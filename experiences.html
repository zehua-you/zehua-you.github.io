<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>Experiences</title>
    <link rel="stylesheet" href="assets/css/experiences.css" />
</head>

<body>
    <!-- NAVBAR -->
    <div class="navigation-bar">
        <nav class="nav-links">
        <a href="index.html">Home</a>
        <a href="hobby.html">Hobby</a>
        <a href="experiences.html" class="active">Experiences</a>
        <a href="publications.html">Publications</a>
        <a href="assets/files/Zehua You CV.pdf" target="_blank">CV</a>
        </nav>
    </div>

    <div class = "experiences-section">
        <div class = "experiences-header">
            Experiences
        <div>
        <div class = "experiences">
            <div class = "experience-block">
                <div class = "experience">
                    Deepfake Perception Analysis – Understanding Human Response to AI Media Sep 2025 – Present
                    Supervised by Dr. Sukrit Venkatagiri, Assistant Professor, Swarthmore College
                    A qualitative research project examining how people perceive and morally evaluate deepfake technology.
                    ➢ Conducted qualitative content analysis on 25 participant interviews (13 transcripts, ~85,000 words) using
                    Atlas.ti to examine emotional, ethical, and societal perceptions of deepfake technology.
                    ➢ Designed and refined a codebook with 4 primary dimensions (consent, authenticity, impact, education)
                    through iterative open and axial coding; achieved intercoder reliability (Cohen’s κ = 0.83) across
                    researchers.
                    ➢ Translated coded data into thematic clusters and conceptual maps linking public perception with regulatory
                    and design frameworks for AI media.
                    ➢ Supporting a broader media ethics and AI perception initiative, contributing insights for future policy
                    recommendations and human-centered AI design.
                </div>
            </div>

            <div class = "experience-block">
                <div class = "experience">
                    TargetPractice – Scam Prevention System Jun 2025 – Aug 2025
                    Supervised by Dr. Sukrit Venkatagiri, Assistant Professor, Swarthmore College
                    An interactive system to train users in scam recognition using multi-agent LLM simulation
                    ➢ Developed TargetPractice with LLM-based scammer–target dialogue simulation (OpenAI API),
                    implementing a custom multi-agent framework among scammer, target, and feedback agents.
                    ➢ Engineered role-specific prompt templates with a rule-based safety layer to enable dynamic yet controlled
                    interactions, ensuring both authenticity and ethical safeguards.
                    ➢ Designed and conducted a controlled experiment (N=150, 4 learning conditions), applying survey
                    instruments and mixed-method statistical analysis (Python/pandas).
                    ➢ Demonstrated significant learning gains, including +8% scam recognition, +19% self-efficacy, and +9%
                    response efficacy compared to baseline.
                </div>
            </div>

            <div class = "experience-block">
                <div class = "experience">
                    Artificial Intelligence Toolkit (AITK) – AI Education Tool Supervised by Dr. Lisa Meeden, Professor and NSE Division Chair, Swarthmore College
                    An accessible toolkit to teach advanced AI concepts through lightweight, reproducible models.
                    Jun 2024 – Aug 2024
                    ➢ Implemented small-scale LLM architectures and word embedding models in Jupyter notebooks, enabling
                    scalable execution both locally and on cloud platforms.
                    ➢ Created custom notebook modules with step-by-step explanations, using the scaled-down models to break
                    down complex AI architectures into approachable, instructional components.
                    ➢ Integrated TensorFlow-based visualization modules to illustrate neural network training dynamics (loss
                    curves, weight evolution), improving transparency of complex processes.
                    ➢ Piloted the toolkit across 5 institutions; user testing reported >75% satisfaction on clarity, usability, and
                    pedagogical value.
                </div>
            </div>

            <div class = "experience-block">
                <div class = "experience">
                    AI Research Institute at GRG Banking Co., Ltd., Guangzhou, China
                    Multimodal Learning Algorithm Researcher Aug 2025 – Sept 2025
                    A robust system to improve information retrieval from video clips, focusing on multimodal learning methods
                    and practical deployment.
                    ➢ Researched foundational and state-of-the-art approaches (e.g., CLIP4ClIP, UCoFiA, Cap4Video) and
                    assessed their suitability for real-world video retrieval applications.
                    ➢ Implemented a Python/PyTorch prototype leveraging a convolutional backbone to extract coarse and fine
                    features, combined with a UCoFiA-inspired cross-modal attention mechanism to enhance alignment.
                    ➢ Designed and initiated the training/evaluation pipeline on the MSR-VTT benchmark dataset with standard
                    retrieval metrics (Recall@K, MnR).
                </div>
            </div>
        </div>
    </div>
</body>
</html>
